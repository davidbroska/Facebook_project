\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk}
\usepackage{cite}
\usepackage{natbib}
\usepackage{dcolumn}
\usepackage{pdfpages}
\usepackage{epigraph} % Required for inspirational quote
% packages required for kable()
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\renewcommand{\epigraphsize}{\small}
\setlength{\epigraphwidth}{0.51\textwidth}
\renewcommand{\textflush}{flushright}
\renewcommand{\sourceflush}{flushright}

% display code
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


%\date{Draft -- please do not circulate.}

\title{(In)civility on Display: Social Media Discourse through the Ideological Lens}

\author[1]{David Broska}
%\author[2]{Byungkyu Lee}
%\author[3]{Barum Park}
%\author[4]{Daniel A. McFarland}
%\affil[1,4]{Stanford University}
%\affil[2]{New York University}
%\affil[2]{Cornell University}

\begin{document}

\maketitle

\epigraph{Civility costs nothing and buys everything.}{Mary Wortley Montagu 1756}
\epigraph{Civility is on the ballot.}{Barack Obama 2016}
%Respect for women is on the ballot. Tolerance is on the ballot. Justice is on the ballot. Equality is on the ballot. Democracy is on the ballot.

\section{Introduction}

Conversations are at the heart of democracies. Discussions in parliaments, the media, and kitchen tables inform political processes, help identify solutions to problems, avoid violent conflict, and foster a better understanding of each other. Some consider conversations based on reason rather than content and ignorance even a benchmark for achieving a democracy \citep{sanders_against_1997}. Attaining these outcomes requires \textit{civility} from the interlocutors because it sets the stage for productive conversations. 
At a minimum,  civility means ``demonstrating mutual respect" \citep{mutz_inyourface_2016} or ``politeness" \citep{frimer_montagu_2018}. A maximalist definition of civility does not only require being respectful and polite but also critical engagement \citep{kant_enlightenment_1784}\footnote{Kant does not only argue that individuals should not only have the freedom but also the courage to make use of their own understanding -- i.e., to actively engage. For example, the clergyman is required to teach lessons aligned with the ``creed of the church he serves, for he was employed by it on that condition. But as a scholar he has complete freedom and is even called upon to communicate to the public all his carefully examined and well-intentioned thoughts about what is erroneous in that creed and his suggestions for a better arrangement of the religious and ecclesiastical body."} and requires individuals to give rationales in conversation \citep{habermas1985theory}. In other words, this expanded definition of civility demands contributions to conversations to be productive above and beyond avoiding offense.

While this distinction is grounded in theoretical traditions, it remains unclear how these perspectives align with the public's perception of desirable norms in conversations. Civility is a \textit{social praxis} embedded in what people know, who they are, what they do, and what they value. Extant literature does not sufficiently address how the broader public applies norms to conversations, and the social and psychological factors that shape the perceptions of conversations are not well understood. \textit{To what extent does the perception of civility depend on individual factors} (e.g. political ideology, partisanship, age, etc.), \textit{the social context} (e.g. the topic of the conversation), \textit{or both} (e.g. some people in some contexts may perceive conversations differently than others in other contexts)? For this manuscript, we will focus on perceived toxicity, defined as a comment considered disrespectful, hateful, or that makes someone more likely to leave a discussion or give up sharing their perspective. 

This project studies the determinants of perceived civility -- and toxicity in particular -- in the context of the 2016 US presidential election campaign. Across 9,994 conversations held between 2015 and 2017 in political Facebook groups, we study how 4,675 American survey respondents evaluate comments given by discussants in terms of perceived toxicity. The 2016 presidential campaign is an insightful case for the study of conversational norms because the discussions happened ``during a time of rapidly changing norms of political civility" \citep{munger_dont_2021}, and these exchanges were experienced ``first-hand" through broadcasting and online \citep{mutz_inyourface_2016}. Studying social media around this time offers a unique lens into the public sphere because an estimated 68\% of US adults were on a social network, up from 25\% in 2008 \citep{duggan_social_2016}.

%While there was once hope that social media could advance democratic deliberation through open discourse, diverse perspectives, and broad participation, many observers note that social media discussions lack the crucial ingredient of civility. Concerns about declining civility are not new. From the printing press during the Reformation \citep{bejan_mere_2017} to the invention of cable news with its personalized news reporting \citep{mutz_inyourface_2016,berry2014outrage}, history is filled with examples of concerns about incivility of discourse. However, the 2016 US presidential election may have marked another critical turning point in recent history. 

\section{Hypotheses}

\textit{Random intercept hypothesis:} Multilevel models allow for assessing how much variation in perceived toxicity is attributable to survey respondents (e.g. political ideology) and to the context (e.g. the topic of a conversation). We hypothesize that a larger share of the variation in perceived toxicity -- the dependent variable in this study -- is attributable to the context than the individual.

\textit{Individual-level hypothesis:} This study is exploratory and considers a broad range of individual predictors of perceived toxicity. That said, perceptions of conversational norms likely depend on demographic factors such as political ideology, partisanship, education, age, race, and religiosity. These demographics reflect socialization processes that shape what is considered appropriate in a conversation. 

\textit{Context-level hypothesis:} Some topics will be considered more toxic than others. In particular, topics related to ``moral convictions"-- i.e., judgments informed by ``perceptions of morality and immorality, right or wrong" \citep{skitka_conviction_2010} -- may elicit particularly strong reactions. Attitudes toward gun control and abortion may be more strongly held than attitudes toward other issues; these topics may create more intense emotions, motivate people to justify their actions, and lead them to believe that their views are objective and universal. 

\textit{Random slope hypothesis:} Not all comments cause variation in perceived toxicity. Some comments may be irrelevant to the observer, too short to elicit strong reactions, or unintelligible. We hypothesize that the random slopes for individual-level characteristics, coupled with random intercepts for topics, will cover zero. Beyond this, it is interesting to assess the range of the most effects (e.g. 95\%). That is, if the effect for an individual-level characteristic is $\hat \beta$, what is $\beta\pm 1.96$?

\textit{Cross-level interaction hypothesis:} Some topics are perceived as more toxic among Republicans/conservatives than liberals, and vice versa. For example, social identity theory predicts that individuals favor members of the ingroup over the outgroup \citep{tafjelturner_identity_2004}. Hence, Democrats may perceive negative characterizations of Hillary Clinton as more toxic than Republicans. Conversely, Republicans might perceive negative comments about Donald Trump as more toxic than Democrats.






%By highlighting the surveying the dimensions of what constitutes perceptions of a civil discourse, our research informs the literature on content moderation \citep{argyle_leveraging_2023}.


% Social scientists have long observed that “conversation is the soul of democracy” (1, 2). Interpersonal discussions across social divides can help diverse groups of people peacefully identify solutions to shared problems, avoid violent conflict, and come to understand one another better (2–8). Historically, these conversations have occurred face-to-face (8), but online conversations now play a central role in public dialogue. More than 100 billion messages are sent every day on Facebook and Instagram alone (9), and approximately 7 billion conversations occur daily on Facebook Messenger (10). Such conversations can have far-reaching impact. Some of the largest social movements in human history have emerged out of sprawling conversations on social media, and discussions between high-profile social media users can shape the stock market, politics, and many other aspects of human experience (11–14). The internet thus has the capacity to empower an ever-increasing number of people to communicate and deliberate together

\section{Data}

To study the determinants of perceived civility in conversations, we recruit 4,675 American adults via Prolific to fill out a survey asking them to evaluate Facebook conversations regarding perceived toxicity and other features. Respondents who did not pass an attention check were not allowed to take the survey. Excluding missing values\footnote{There is no evidence of differential attrition.}, the total sample size is 32,597 observations. Those who passed the attention check were randomly assigned to evaluate 7 conversations. Not every respondent evaluated each conversation; each one was evaluated 3.27 times on average. 

The 9,994 conversations are a stratified random sample of conversations from 1,058 public Facebook pages retrieved during the 2016 US presidential campaign via the official Facebook API v2.7. To ensure variation across the ideological spectrum on Facebook, we first obtained a list of the 500 most active political pages of the platform according to \citet{bakshy_exposure_2015}, as well as 37 pages identified as representative online and offline news sources by Pew Research Center’s American Trends Panel in 2014. These pages make up our media samples.

Table \ref{tab:summ-tab-ind} in the Supplementary Material provides summary statistics of the characteristics of survey respondents. Compared to demographic quotas from the American National Election Studies (ANES), this sample of 4,675 adults recruited via Prolific consists of more college-educated individuals (+10.7 percentage points) than those with a high school degree or less (-7.3pp). We over-sampled Democrats (+10.7pp) and under-sampled Republicans (-14.6pp). Finally, we over-sampled those aged 33 to 44 (+9.4pp), and under-sampled those aged 60 and older (-12.3pp). The sample is otherwise representative on these demographic dimensions. In a future analysis, we will consider whether results change if post-stratification techniques are applied, allowing us to make more credible inferences about the population of American adults.


Table \ref{tab:summ-tab-topics} provides summary statistics of variables measured for conversations; the first set of variables in the \textit{Topic} category refers to categorizations of whether a particular comment is related to a topic or not. The Facebook comments were classified with a Large Language Model (LLM) to create predictor variables for the regression analysis (please refer to Figure \ref{fig:intercoder-reliability} for more details). The \textit{Other} variables denote predictors unrelated to the language of the comment.

%\begin{itemize}
%    \item \textit{Order:} how many conversations a respondent evaluate before rating the focal one
%    \item \textit{BToxicNum01:} toxicity of B's comment rated by survey respondents. The four levels Not Toxic (-1), Maybe not sure (0), Toxic (1), and Very toxic (2) were rescaled to range from 0 to 1 and will be treated as a continuous dependent variable.
%    \item \textit{ideocommenterB}: the political ideology of commenter B is estimated based on the Facebook pages that the user visited. Negative values denote liberal political ideology while positive values represent conservatism.
%\end{itemize}

Table \ref{tab:summ-tab-emf} shows summary statistics on sentiment scores assigned to commenter B's comment assigned with the expanded Moral Foundations Dictionary \citep{hopp2021extended}.\footnote{"Each word is assigned 5 sentiment scores that denote the average sentiment of the foundation context in which this word appeared. For example, the word 'kill' has an average 'care_sent' of -0.69, meaning that all 'care-harm' highlights in which 'kill' appeared had an average, negative sentiment of -0.69." 

https://github.com/medianeuroscience/emfdscore/blob/master/eMFDscore_Tutorial.ipynb}
Table \ref{tab:summ-tab-liwc} presents the word frequencies per category in the LIWC dictionary \citep{pennebaker_linguistic_2022}. The counts were transformed with $\log(1+x)$ to account for the right-skew of count variables. These dictionary counts will be included as control variables but they do not represent the focus of the analysis.

\section{Methods}

Multilevel models, also known as mixed-effects models, are statistical models used for analyzing data that have a nested structure (see Table \ref{sample-table}  for illustration). This means that the data is organized into groups, and there is variability both within each group and between groups \citep{hox2017multilevel}. Multilevel models can account for correlation within each group. In the present case, evaluations of conversations may be correlated within each individual (because it is the same rater), and conversations may be evaluated similarly (because it is the entity). This does not imply a strictly hierarchical structure since each respondent evaluated multiple conversations but not all conversations have been evaluated by the same respondents. Hence, these data are cross-classified.

\section{Results}

In the following, I will refer to the models reported in the table on pp.13.

\textit{Random intercept hypothesis:} The intraclass correlation indicates that approximately 11\% of the variation in perceived toxicity is attributable to individual factors whereas 39\% of the variation comes from features of the comments (see Model 1). 

\textit{Individual-level hypothesis:} Several individual-level predictors are associated with perceived toxicity. For example, a 2 standard deviation increase in political ideology -- i.e., an increase of approximately 3.5 points on a 1 to 7 scale corresponding to a shift from extremely liberal to moderate or from moderate to extremely conservative -- is associated with an approximate decrease of 6 percentage points in average perceived toxicity, controlling for other variables in the model. More broadly, this association suggests that liberals are more sensitive while conservatives are less so. The magnitude of this association is comparable to a 20-year increase in age. While older folks tend to be less sensitive, more educated people tend to rate comments as more toxic on average. Finally, the coefficient for \textit{Order} suggests that processes by which people get numbed partially explain insensitivity. Having been exposed to six comments before rating the 7th is comparable to the magnitude of the association with political ideology. In total, the individual-level predictors in Model 2 explain 9\% of the variance at the individual level -- i.e. 9\% of 11\%.

\textit{Context-level hypothesis:} The comments on the following topics are considered significantly more toxic (example Facebook comments in parenthesis): 
\begin{itemize}
    \item Negative characterization of America (e.g. ``The US is governed by incompetent politicians")
    \item Suggestive content (e.g. ``Prostitutes will be like ...")
    \item Negative characterization of Hillary Clinton (e.g. ``Hillary is a liar and she will never be president of the United States")
    \item Drugs (e.g. ``She's high on drugs of stupidity", ``And who's fault is it that the cartels exist?? Your people")
    \item Negative characterization of immigrants (e.g. ``The immigrants steal our jobs")
\end{itemize}

Only two topics are perceived as significantly less toxic on average.

\begin{itemize}
    \item Positive characterization of Hillary Clinton (e.g. ``During the debates, Hillary will mop Donald Trump off of the floor and him into the sewer because she is 20x smarter")
    \item Positive characterization of immigrants (e.g. ``Most immigrants are actually highly educated doctors, lawyers, and computer engineers")
\end{itemize}

Comparing the a model only with random intercepts against model that also includes the topic predictors (not shown in table), we find that including the 14 topics explains about 13.7\% of the variance in perceived toxicity at the conversation level -- i.e. 13.7\% of the 39\%. 



\textit{Random slope hypothesis:} According to model 4,\footnote{Model 4 did not converge when using both random slopes and control variables.} the standard deviation of the random slopes for political ideology estimated for each comment is 0.085. Based on this, we estimate that 95\% of the slopes are between $-0.065 - 1.96\times 0.085=-0.23$ and  $-0.0645 + 1.96 \times 0.085=0.10$. In line with our hypothesis above, this means that the relationship between political and perceived toxicity is zero for some comments. In other words, some comments do not elicit a reaction across the ideological spectrum. These comments might be too short, unintelligible, or uncontroversial.

\textit{Cross-level interaction hypothesis:} Model 5 estimates interactions effects. For a given topic, a negative effect means that conservatives perceive a comment as less toxic than liberals. Conservatives find comments that endorse abortion, immigration, Hillary Clinton, and that characterize Trump negatively significantly more toxic than liberals. Conservatives perceive comments related to anti-America rhetoric, pro-Trump, anti-Clinton, and anti-abortion as less toxic. In sum, I find support for the hypothesis that the association between political ideology and perceived toxicity depends on the topic. These interaction effects were estimated in a single model. Estimates from separate models are very similar in terms of the sign and statistical significance of the coefficients (see Table \ref{tab:interaction-effects-comparison}).

\section{Conclusion and Discussion}

Conversations are a crucial element in the democratic process. If someone perceives a conversation as toxic, they might be reluctant to share their perspective or disengage altogether. This study offers preliminary results on the determinants of perceived toxicity in conversations. We find that more variation in perceived toxicity is attributable to the features of comments than to individual-level factors (approximately 3.5 times more variation). Across comments, liberals are more sensitive than conservatives, and this effect seems sizable compared to other predictors' effects. Why are conservatives less sensitive? On average, conservatives have been exposed to more irritating content on social media more often than liberals. For example, \citet{bakshy_exposure_2015} find that conservatives encounter more content that cuts across ideological lines than liberals through friends, their user feed, and the content they click on. That said, the relationship between political ideology and perceived toxicity is not uniform. Both conservatives and liberals are more sensitive when it comes to certain issues. In line with social identity theory, for instance, liberals perceive negative comments on Hillary Clinton as more toxic and conservatives are more sensitive towards negative portrayals of Donald Trump. 

While these results are promising, we would be remiss not to mention this work's shortcomings. Toxicity is only one dimension of civility and similar analyses can be conducted for other dependent variables in this dataset, including perceived hate speech or perceived productivity of a conversation. A similar point can be made about the independent variables in this study; many more deserve closer attention than was given in this short report. Last, this analysis has raised more questions than it provides answers. What are the social and psychological processes that underlie differences in perceived toxicity between liberals and conservatives? We will address this question in an expanded version of this research project. 

\newpage
\bibliographystyle{asr}
\bibliography{bib}

\clearpage
\section{Supplementary Material}

\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Respondent Id} & \textbf{Post Id} & \textbf{Comment A Id} & \textbf{Comment B Id} \\
    \hline
    R1 & P2 & A2 & B2 \\
    R1 & P4 & A4 & B4 \\
    R1 & P23 & A23 & B23 \\
    R1 & P5 & A5 & B5 \\
    R1 & P56 & A56 & B56 \\
    R1 & P6 & A6 & B6 \\
    \midrule
    R3 & P4 & A4 & B4 \\
    R3 & P22 & A22 & B22 \\
    R3 & P6 & A6 & B6 \\
    R3 & P7 & A7 & B7 \\
    R3 & P9994 & A9994 & B9994 \\
    R3 & P11 & A11 & B11 \\
    R3 & P55 & A55 & B55 \\
    \midrule
    \dots & \dots & \dots & \dots \\
    \hline
    \end{tabular}
    \caption{Sample table to illustrate the nested data structure}
    \label{sample-table}
\end{table}

\clearpage

% Dependent variable plot 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/2_AvgAgreement_Partisanship.pdf}
    \caption{Variables measuring dimensions of civility by partisanship}
    \label{fig:civility-partisanship}
    \vspace{0.25cm}
    \includegraphics[width=1\linewidth]{figures/2_AvgAgreement_Ideo.pdf}
    \caption{Variables measuring dimensions of civility by political ideology}
    \label{fig:civility-ideology}
\end{figure}
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Summary statistics % 
\renewcommand{\arraystretch}{0.9}
\input{Tables/summary_stats_ind}

\input{Tables/summary_stats_topics}

\input{Tables/summary_stats_emf}
\input{Tables/summary_stats_liwc}
\clearpage
% End summary statistics 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% Regression table 
\clearpage
\includepdf[pages={1-},scale=0.95]{tox.html.pdf}




% Model comparison interaction effects

The interaction effects reported in the main text were estimated with a single model. To assess the robustness of these results, these effects were also estimated with separate models. Table \ref{tab:interaction-effects-comparison} shows that in all but four cases the interaction effects estimates had the sign and were either both significant or both insignificant. Additionally, in all but one case (\textit{antitrump}), the effects had the same sign.

\include{Tables/InteractionEffectsIndVsAll}
\clearpage

\clearpage

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/corrplot_topics.pdf}
    \caption{Correlation plot. The small Pearson correlations between topics indicate that topics capture distinct dimensions with little overlap. This occurs because most comments are short, communicating only one or few issues.}
    \label{fig:corrplot-topics}
\end{figure}
\clearpage

% Intercoder reliability
\begin{figure}
    \centering
    \includegraphics[width=1.3\linewidth]{figures/classifier_performance.pdf}
    \caption{Inter-coder reliability for classifying a comment as either relevant or irrelevant to a topic. Reliability is calculated for two human annotators and one LLM (Mistral 8GB). For example, the dark green dots indicate reliability when human annotator DB is considered the 'ground truth'., and these annotated labels are predicted by the LLM (light green) or the human annotators NE (dark red). In general, the LLM accurately identifies comments irrelevant to a topic (near-perfect specificity) and delivers acceptable performance in identifying all relevant comments (recall). The LLM does not perform well at discerning between relevant and irrelevant comments -- i.e., avoiding false positives (low precision). Given the small number of true positives, we will manually label all cases labeled as relevant by the LLM, thereby maximizing specificity.}
    \label{fig:intercoder-reliability}
\end{figure}
\clearpage

%\include{written/codebook}

\textbf{Example prompt: Pro-immigration}

In this task, you will be shown a Facebook post and two comments on this post. One comment is from person A and the other comment is from person B. 

Your job is to determine if the comment by B is related to the positive characterization of immigrants, individuals who relocate from their native country to another. This involves portraying the immigrants in a favorable light, highlighting perceived strengths, qualities, and virtues.
Your answer must be a single number: 0 if irrelevant, 1 if relevant.

Here are some examples with reasoning. Use the following format for your output:

Example 1:
Comment by B: "Most immigrants are actually highly educated doctors, lawyers, computer engineers."
Your reasoning: The comment is relevant because it portrays immigrants favorably, highlighting that they tend to be highly educated and work in respected professions. 
Your answer: 1

Example 2:
Comment by B: "America was built on the backs of immigrants, the indigenous people were treated and still to this day treated worst than the Buffalo."
Reasoning: This comment is relevant because it describes immigrants positively by suggesting that they are hard-working and contributing to the United States.
Your answer: 1

Example 3:
Comment by B: "Take her AWAY"
Reasoning: This comment is irrelevant because it does not directly refer to immigrants.



\clearpage

\end{document}